사실 이번 장을 다 읽었을 때 우리가 키 값 저장소를 설계해야 할 일이 있을까가 제일 의문이 들었다. 

그래서 일단 제일 먼저 GPT에게 물었다. GPT가 제시한 상황은 아래와 같다.

1. **특수한 요구 사항을 충족해야 할 때:** 기존의 키-값 저장소 시스템이 프로젝트 또는 응용 프로그램의 요구 사항을 충족시키지 못하는 경우. 예를 들어, 더 빠른 읽기 또는 쓰기 성능이 필요한 경우, 데이터 모델을 특별히 맞춤화해야 하는 경우 등.
2. **데이터 모델이나 스키마가 독특한 경우:** 기존의 키-값 저장소 시스템이 지원하지 않는 데이터 모델이나 스키마를 사용해야 할 때. 예를 들어, 그래프 형식의 데이터를 저장하거나, 복잡한 데이터 구조를 다루어야 할 때.
3. **확장성과 고가용성이 필요한 경우:** 특히 대규모 시스템에서는 확장성과 고가용성이 중요합니다. 기존 솔루션으로 이러한 요구 사항을 충족시키기 어려울 때 직접 구축하는 것이 유용할 수 있습니다.
4. **특정 보안 요구 사항이 있는 경우:** 민감한 데이터를 다루는 경우, 보안 요구 사항을 정확히 충족시키기 위해 특별한 설계가 필요할 수 있습니다.
5. **비용 효율성을 고려할 때:** 상용 키-값 저장소 솔루션의 라이선스 비용이나 서비스 비용을 절약하려는 경우 직접 구축할 수 있습니다.
6. **특별한 인프라 환경이 있는 경우:** 특정 하드웨어 또는 네트워크 환경에 최적화된 키-값 저장소를 구축해야 하는 경우.
7. **커스터마이징이 필요한 경우:** 일반적인 키-값 저장소 솔루션으로는 요구 사항을 충족시키기 어려운 특별한 비즈니스 로직 또는 기능을 구현해야 하는 경우.

직접 설계 할 일이 있을 지 잘 이해가 가지 않았지만, 그렇다고 합니다…

### CAP 정리

순서대로 데이터 이로간성, 가용성, 파티션 감내(분할 용인)라는 특성을 모두 동시에 만족하는 분산 시스템은 설계할 수 없다라는 정리이다. 다른 말로, 브루어의 정리라고도 한다.

여기서 P 즉, Partition tolerance라는 특성에 대해 정확한 정의가 없다고 한다.

CAP 정리에 따르면 마치 C,A,P가 동등한 선상에 있는 것처럼 느껴지지만, 사실은 C와 A는 분산시스템의 특성이고, P는 그 분산시스템이 돌아가는 네트워크에 대한 특성이다. P의 선택을 배제하려면, 장애가 나지 않는 네트워크를 구성해야 한다. 하지만 장애가 나지 않는 네트워크를 존재하지 않으므로, 결국 선택권은 C나 A 둘중 하나밖에 없다. (또한 C와 A의 관계가 비대칭적이라고 하는데,, P는 무조건 선택할 수 밖에 없는 선택지 이므로, C와 A는 트레이드오프 관계라고 하는 것 같습니다.)

그래서 이러한 문제점을 보완하고자 PACELC이라는 개념이 나왔다.

### PACELC(Patition→ A↔B,else → L ↔C)

CAP는 장애 상황일 때의 선택에 대해 서술하는 것으로 생각하면, 정상 상황일 때의 선ㅌ택에 대해 서술하지 못하게 된다. 그래서 정상 상황일 때와 장애 상황일 때를 나누어 설명하고자 하는 것이 PACELC의 핵심이다. PACELC의 의미를 인용하자면 아래와 같다.

> *if there is a partition (P) how does the system tradeoff between availability and consistency (A and C); else (E) when the system is running as normal in the absence of partitions, how does the system tradeoff between latency (L) and consistency (C)?*
> 
- 파티션(네트워크 장애) 상황일 때에는 A와 B는 trade-off 관계이고, 그외의(정상) 상황일 때에는 L과 B가 trade-off 관계이다.
    - 장애상황일 때에는 CAP정리에서의 이유와 마찬가지로 C와 A 둘 중 하나만 선택할 수 있다.
    - 정상상황일 때에는 모든 노드에 데이터를 반영하기에는 지연시간이 커질 수 밖에 없다. 지연시간을 낮추기 위해서는 모든 노드에 데이터를 반영하는 것을 포기해야 한다.

![image](https://github.com/junchanpp/2023-system-design-interview-2nd/assets/49396352/cc3ffe0a-e3c5-4c76-a377-863fc6bb7e9f)


- PACELC 구성
    
    
    | 구분 | 구성 | 설명 |
    | --- | --- | --- |
    | Partition | Availability | 가용성 |
    |  | Consistency | 일관성 |
    | Else | Latency | 시간 지연 |
    |  | Consistency | 일관성 |
- 기존 DB에 대입
    
    ![image](https://github.com/junchanpp/2023-system-design-interview-2nd/assets/49396352/ab87b370-1946-4dc8-b63e-322e41eca19f)

    
    - PC/EC(Megastore, VoltDB)
        
        언제나 일관성을 위해 A나 L를 포기한다.
        
    - PA/EL(Dynamo, Cassandra)
        
        장애 상황일 때에는 가능한(장애가 아닌) 노드에만 데이터를 반영하고, 정상으로 복구되면 필요한 노드에 데이터를 모두 반영한다. 정상상황일 때에도 Latency를 위해 모든 노드에 데이터를 반영하지 않는다.
        
    - PA/EC(MongoDB)
        
        장애 상황일 때에는 일관성을 포기하고, 가용성을 선택한다. 장애 상황이 복구되면(정상상황) 이를 감지하여 일관성 보장을 위해 모든 노드에 데이터를 반영한다.
        
    - PC/EL(PNUTS)
        
        장애 상황일 때에는 일관성을 위해 가용성을 포기한다. 평소에는 지연속도 낮추기 위해 일관성을 포기한다. → 이해가 잘 안되지만, 여기서 PC의 의미는 “평소만큼의 C(지연속도를 낮추기 위해 어느 정도 포기한 일관성만큼)를 보장하기 위해 A를 희생시킨다.”라는 의미이다. 일반적인 일관성보다는 약하다.(아래의 정족수 합의 프로토콜과 연관지어 생각하면 이해가 쉬울 것이다.)
        

[CAP Theorem, 오해와 진실](http://eincs.com/2013/07/misleading-and-truth-of-cap-theorem/)

[[Database] CAP 정리 - Azderica](https://azderica.github.io/00-db-cap/)

### 데이터 파티션

데이터를 파티션단위로 나눌 때에는 다음 두가지 문제를 따져봐야 한다.

1. 데이터를 여러 서버에 고르게 분산할 수 있는가?
2. 노드가 추가되거나 삭제될 때 데이터의 이동을 최소화할 수 있는가?

→ 5장에서 배운 안정 해시를 이용하자.

이를 통해 규모확장 자동화와 다양성 면에서 이점이 챙길 수 있다.

### 데이터 다중화

높은 가용성과 안정성을 확보하기 위해 같은 데이터를 N개의 서버에 비동기적으로 다중화해야 한다. 가상 노드의 경우 같은 물리 노드를 중복 선택하지 않도록 해야 한다.

### 데이터 일관성

여러 노드에 다중화된 데이터는 적절히 동기화가 되어야 한다. 이것은 정족수 합의 프로토콜을 사용하여 읽기/쓰기 연산 모두에 일관성을 보장할 수 있다.

- 정족수 합의 프로토콜
    
    [분산 환경에서의 일관성 처리: 정족수 합의](https://seungjuitmemo.tistory.com/296)
    

### 일관성 불일치 해소

데이터 버저닝과 벡터 시계를 통해 해소가 가능하다. 버저닝은 데이터가 변경할 때마다 해당 데이터의 새로운 버전을 만드는 것이다. 따라서 각 버전의 데이터는 불변하다. 벡터 시계는 [서버,버전] 순서쌍이다. 어떤 버전이 선행 버전인지, 후행 버전인지, 아니면 다른 버전과 충돌이 있는지 판별한다. 단점은 아래와 같이 두 가지가 있다.

1. 충돌 감지 및 해소 로직이 클라이언트에 들어가야 하므로, 클라이언트 구현이 복잡해진다.
2. [서버, 버전]의 순서쌍 개수가 굉장히 빨리 늘어난다.
    
    → 이것은 실제로 문제가 벌어진 적이 없으므로 유의미한 단점은 아닌 듯함.
    

### 장애 처리

장애 감지 → 장애 해소

### 장애 감지

분산 시스템에서는 한 대의 서버가 “특정 서버가 죽었다”라고 해서 바로 장애라고 판별하지 않고, 두 대 이상의 서버에서 보고해야 실제로 장애가 발생했다라고 판단한다. 

멀티캐스팅 채널을 구축하는 것이 서버 장애를 감지하는 손쉬운 방법이지만, 서버가 많을 때에는 비효율적이다.

가십 프로토콜같은 분산형 장애 감지 솔루션을 채택하는 것이 효율적이다.

**가십 프로토콜**

- 마치 바이러스가 퍼지는 방식과 유사하게 동작하기 때문에 전염병 프로토콜이라고도 한다.
- 가십은 하나의 합의를 보는 게 아니라 다수의 노드로부터 인증받아 하나의 합의를 이끌어 내는 것이다.
- 마스터가 없는 대신 각 노드가 주기적으로 통신을 통해 서로 메타 정보를 주고 받는다.
- 낮은 latency와 낮은 data load를 통해 장애를 감지 할 수 있다.(하지만 Multicast 데이터를 갖고 있는 모든 노드들이 한 번에 동시에 실패한다면 장애 감지 실패하게 된다. 그런 경우는 거의 없지만..)

[Gossip 프로토콜이란?](https://medium.com/@heonjang.lee96/gossip-프로토콜이란-906500c3de4b)

[가십 - 해시넷](http://wiki.hash.kr/index.php/가십)

가십 프로토콜의 동작 원리는 다음과 같다.

1. 각 노드는 멤버십 목록을 유지한다. 멤버십 목록은 각 멤버 ID와 그 박동 카운터 쌍의 목록이다.
2. 각 노드는 주기적으로 자신의 박동 카운터를 증가시킨다.
3. 각 노드는 무작위로 선정된 노드들에게 주기적으로 자기 박동 카운터 목록을 보낸다.
4. 박동 카운터 모록을 받은 노드는 멤버십 목록을 최신 값으로 갱신한다.
5. 어떤 멤버의 박동 카운터 값이 지정된 시간 동안 갱신되지 않으면 해당 멤버는 장애 상태인 것을 간주한다.

**스윔 프로토콜**

[CC 03: Membership Protocol](https://1ambda.github.io/cloud-computing/cloud-computing-3/)

https://www.tokenpost.kr/article-111303

![image](https://github.com/junchanpp/2023-system-design-interview-2nd/assets/49396352/aa01dba2-3d74-4c85-8ded-54880be30c2e)
- 위의 첫번째 글을 읽고 가십 프로토콜의 단점과 가십 프로토콜을 대체할 수 있는 스윔프로토콜이 있다는 것을 알았지만,,, 두번째 글을 보면 지원 중단했다고 해서 추가적으로 알아보지 않았습니다.
- 간단하게 Process load와 first detction time 사이에 trade-off가 있다는 정도만 알고 넘어갔습니다.

### 장애 해소

**일시적 장애 처리**

가십 프로토콜로 장애를 감지한 시스템은 가용성을 보장하기 위해 필요한 조치를 해야 한다. 느슨한 정종수 접근법으로 가용성을 높일 수 있다. 정족수 요구사항을 강제하는 대신, 장애 상태인 서버를 무시하고  건강한 W개의 서버와 R개의 서버를 해시링에서 고른다.

네트워크나 서버 문제로 장애 상태인 서버로 가는 요청은 다른 서버가 잠시 맡아 처리한다. 그동한 발생한 변경사항은 해당 서버가 복구 되었을 때 일괄 반영하여 데이터 일관성을 보존한다. 이를 위해 임시로 쓰기 연산을 처리하는 서버에는 그에 관한 단서를 남겨둔다 ← 단서 후 임시 위탁 기법이라 부른다.

**영구 장애 처리**

영구적인 노드의 장애는 반-엔트로피 프로토콜을 구현하여 사본들을 동기화한다. 이 프로토콜은 사본들을 비교하여 최신 버전으로 갱신하는 과정을 포함한다. 사본 간의 일관성이 망가진 상태를 탐지하고 전송 데이터의 양을 줄이기 위해서는 머클 트리를 사용한다. 이를 통해 꼭 필요한 버킷만 동기화 작업을 함으로써, 동기화 해야 하는 데이터를 최소화할 수 있다.
